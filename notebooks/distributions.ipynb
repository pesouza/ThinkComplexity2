{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where Do Distributions Come From?\n",
    "\n",
    "Code examples from [Think Complexity, 2nd edition](https://thinkcomplex.com).\n",
    "\n",
    "Copyright 2016 Allen Downey, [MIT License](http://opensource.org/licenses/MIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import empiricaldist\n",
    "except ImportError:\n",
    "    !pip install empiricaldist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print('Downloaded ' + local)\n",
    "    \n",
    "download('https://github.com/AllenDowney/ThinkComplexity2/raw/master/notebooks/utils.py')\n",
    "\n",
    "from utils import decorate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Distribution\n",
    "\n",
    "The normal distribution, also known as the Gaussian distribution, is a good model for many of the distributions we see in natural and engineered systems.\n",
    "\n",
    "For example, the distribution of birthweights is approximately Gaussian. Here's data from the National Survey of Family Growth we can use to test this claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('https://github.com/AllenDowney/ElementsOfDataScience/raw/master/data/nsfg.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nsfg = pd.read_hdf('nsfg.hdf', 'nsfg')\n",
    "nsfg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birthweight is encoded in two variables, which we have to clean and then combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfg['BIRTHWGT_LB1'].replace([98, 99], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfg['BIRTHWGT_OZ1'].replace([98, 99], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthweight = nsfg['BIRTHWGT_LB1'] + nsfg['BIRTHWGT_OZ1'] / 16\n",
    "birthweight.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize the distribution of birthweights is a KDE plot, which uses kernel density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(birthweight)\n",
    "\n",
    "decorate(xlabel='Birthweight (lb)',\n",
    "         ylabel='Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical axis is density, which is a relative likelihood for each weight.\n",
    "\n",
    "We can see that this distribution is approximately bell-shaped, which is the characteristic shape of the Gaussian distribution.\n",
    "\n",
    "But it is not easy to tell, by looking at a KDE plot, whether the Gaussian distribution is a good model for the data.\n",
    "A better option is to plot the CDF of the data and compare it to the CDF of a Gaussian distribution with the same mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empiricaldist import Cdf\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_normal_cdf(sample, **options):\n",
    "    mean, std = sample.mean(), sample.std()\n",
    "    low, high = sample.min(), sample.max()\n",
    "\n",
    "    xs = np.linspace(low, high)\n",
    "    ys = norm.cdf(xs, mean, std)\n",
    "\n",
    "    plt.plot(xs, ys, color='gray', alpha=0.5, label='Normal model')\n",
    "\n",
    "    cdf = Cdf.from_seq(sample)\n",
    "    cdf.plot(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(birthweight, label='NSFG data')\n",
    "\n",
    "decorate(xlabel='Birthweight (lb)',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CDF of a Gaussian distribution has a recognizable sigmoid shape. And by comparing it with the data, we can see where the model fits well and where it deviates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think the Gaussian model is a good choice for a particular dataset, one way to check is a [normal probability plot](https://en.wikipedia.org/wiki/Normal_probability_plot), which plots the observed data against randomly-generated data from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_probability_plot(sample, fit_color='0.8', **options):\n",
    "    \"\"\"Makes a normal probability plot with a fitted line.\n",
    "\n",
    "    sample: sequence of numbers\n",
    "    fit_color: color string for the fitted line\n",
    "    options: passed along to Plot\n",
    "    \"\"\"   \n",
    "    n = len(sample)\n",
    "    mean, std = np.mean(sample), np.std(sample)\n",
    "\n",
    "    xs = np.random.normal(mean, std, n)\n",
    "    xs.sort()\n",
    "    \n",
    "    plt.plot(xs, xs, color=fit_color, label='model')\n",
    "\n",
    "    ys = np.array(sample, copy=True)\n",
    "    ys.sort()\n",
    "    \n",
    "    plt.plot(xs, ys, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_plot(birthweight)\n",
    "decorate(xlabel='Random Gaussian values',\n",
    "         ylabel='Birthweight (lb)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of a normal probability plot is that it shows deviations from the normal model more clearly, especially in the tails.\n",
    "\n",
    "The drawback is that it might show deviations *too* clearly -- a model that's good enough for practical purposes might look pretty bad on a normal probability plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** One reason the distribution of birthweight deviated from the Gaussian model is that it includes some very light babies that were born pre-term, which is commonly defined to mean prior to the 36th week of pregnancy.\n",
    "\n",
    "The following code selects the birthweights of babies who were born full term. Run this code, and then go back to run the visualizations in this section. Is the distribution of birthweights for full-term babies closer to a Gaussian model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_term = nsfg['PRGLNGTH'] >= 36\n",
    "birthweight = birthweight[full_term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "One reason the Gaussian distribution is a good model for so many think we see in the real world is the Central Limit Theorem, which says:\n",
    "\n",
    "> If you add up independent values from a distribution with finite mean and variance, the sum converges on a normal distribution.\n",
    "\n",
    "To demonstrate, the following function generates samples with difference sizes from an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_expo_samples(beta=2.0, iters=1000):\n",
    "    \"\"\"Generates samples from an exponential distribution.\n",
    "\n",
    "    beta: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: map from sample size to sample\n",
    "    \"\"\"\n",
    "    samples = {}\n",
    "    for n in [1, 10, 100]:\n",
    "        sample = [np.sum(np.random.exponential(beta, n))\n",
    "                  for _ in range(iters)]\n",
    "        samples[n] = sample\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates normal probability plots for samples with various sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_plot_samples(samples, ylabel=''):\n",
    "    \"\"\"Makes normal probability plots for samples.\n",
    "\n",
    "    samples: map from sample size to sample\n",
    "    label: string\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    plot = 1\n",
    "    for n, sample in samples.items():\n",
    "        plt.subplot(1, 3, plot)\n",
    "        plot += 1\n",
    "        \n",
    "        normal_probability_plot(sample)\n",
    "\n",
    "        decorate(title='n=%d' % n,\n",
    "                 xticks=[],\n",
    "                 yticks=[],\n",
    "                 xlabel='Random Gaussian values',\n",
    "                 ylabel=ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows how the sum of exponential variates converges to normal as sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_expo_samples()\n",
    "normal_plot_samples(samples, ylabel='Sum of expo values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential distribution is not very heavy tailed, so the number of addends doesn't have to be very big before the sum is well modeled by a Gaussian distribution.\n",
    "\n",
    "The lognormal distribution has higher variance, so it requires a larger sample size before it converges to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lognormal_samples(mu=1.0, sigma=1.0, iters=1000):\n",
    "    \"\"\"Generates samples from a lognormal distribution.\n",
    "\n",
    "    mu: parmeter\n",
    "    sigma: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"\n",
    "    samples = {}\n",
    "    for n in [1, 10, 100]:\n",
    "        sample = [np.sum(np.random.lognormal(mu, sigma, n))\n",
    "                  for _ in range(iters)]\n",
    "        samples[n] = sample\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_lognormal_samples()\n",
    "normal_plot_samples(samples, ylabel='sum of lognormal values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pareto distribution has infinite variance, and sometimes infinite mean, depending on the parameters.  It violates the requirements of the CLT and does not generally converge to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pareto_samples(alpha=1.0, iters=1000):\n",
    "    \"\"\"Generates samples from a Pareto distribution.\n",
    "\n",
    "    alpha: parameter\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"\n",
    "    samples = {}\n",
    "\n",
    "    for n in [1, 10, 100]:\n",
    "        sample = [np.sum(np.random.pareto(alpha, n))\n",
    "                  for _ in range(iters)]\n",
    "        samples[n] = sample\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_pareto_samples()\n",
    "normal_plot_samples(samples, ylabel='sum of Pareto values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the random variates are correlated, that also violates the CLT, so the sums don't generally converge.\n",
    "\n",
    "To generate correlated values, we generate correlated normal values and then transform to whatever distribution we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlated(rho, n):\n",
    "    \"\"\"Generates a sequence of correlated values from a standard normal dist.\n",
    "    \n",
    "    rho: coefficient of correlation\n",
    "    n: length of sequence\n",
    "\n",
    "    returns: iterator\n",
    "    \"\"\"\n",
    "    x = np.random.normal(0, 1)\n",
    "    yield x\n",
    "\n",
    "    sigma = np.sqrt(1 - rho**2)\n",
    "    for _ in range(n-1):\n",
    "        x = np.random.normal(x * rho, sigma)\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import expon\n",
    "\n",
    "def generate_expo_correlated(rho, n):\n",
    "    \"\"\"Generates a sequence of correlated values from \n",
    "    an exponential dist.\n",
    "\n",
    "    rho: coefficient of correlation\n",
    "    n: length of sequence\n",
    "\n",
    "    returns: NumPy array\n",
    "    \"\"\"\n",
    "    normal = list(generate_correlated(rho, n))\n",
    "    uniform = norm.cdf(normal)\n",
    "    expo = expon.ppf(uniform)\n",
    "    return expo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correlated_samples(rho=0.9, iters=1000):\n",
    "    \"\"\"Generates samples from a correlated exponential distribution.\n",
    "\n",
    "    rho: correlation\n",
    "    iters: number of samples to generate for each size\n",
    "\n",
    "    returns: list of samples\n",
    "    \"\"\"    \n",
    "    samples = {}\n",
    "    for n in [1, 10, 100]:\n",
    "        sample = [np.sum(generate_expo_correlated(rho, n))\n",
    "                  for _ in range(iters)]\n",
    "        samples[n] = sample\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_correlated_samples()\n",
    "normal_plot_samples(samples, \n",
    "                    ylabel='Sum of correlated exponential values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lognormal Distributions\n",
    "\n",
    "The following cells download and read data from the [Behavior Risk Factor Surveilance System](https://www.cdc.gov/brfss/index.html) (BRFSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('https://github.com/AllenDowney/ElementsOfDataScience/raw/master/data/brfss.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brfss = pd.read_hdf('brfss.hdf', 'brfss')\n",
    "brfss.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the columns contains self-reports weights in kilograms for a representative sample of adult residents in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = brfss['WTKG3']\n",
    "weight.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are different for men and women, so I'll partition them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = brfss['_SEX'] == 1\n",
    "female = brfss['_SEX'] == 2\n",
    "\n",
    "weight_male = brfss.loc[male, 'WTKG3']\n",
    "weight_female = brfss.loc[female, 'WTKG3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether adult weights are well modeled by a Gaussian distribution, we'll compare CDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(weight_male, label='Male')\n",
    "plot_normal_cdf(weight_female, label='Male')\n",
    "\n",
    "decorate(xlabel='Adult weight (kg)',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make a normal probability plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_plot(weight_male)\n",
    "\n",
    "decorate(xlabel='Random Gaussian values',\n",
    "         ylabel='Adult weight (kg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** If we think a distribution might be lognormal, one way to check is to compute the log of the values and test whether they fit a normal distribution.\n",
    "\n",
    "Compute the log of adult weight for men and women and run the tests in this section with the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Where Do Lognormal Distributions Come From?\n",
    "\n",
    "It seems that the distribution of adult weights is approximately lognormal. One possible explanation is that the weight a person gains each year is proportional to their current weight. In that case, adult weight is the product of a large number of multiplicative factors:\n",
    "\n",
    "```\n",
    "w = w0 f1 f2 ... fn  \n",
    "```\n",
    "\n",
    "where `w` is adult weight, `w0` is birth weight, and `fi` is the weight gain factor for year `i`.\n",
    "\n",
    "The log of a product is the sum of the logs of the factors:\n",
    "\n",
    "```\n",
    "logw = log w0 + log f1 + log f2 + ... + log fn \n",
    "```\n",
    "\n",
    "So by the Central Limit Theorem, the distribution of `logw` is approximately normal for large `n`, which implies that the distribution of `w` is lognormal.\n",
    "\n",
    "To model this phenomenon, choose a distribution for `f` that seems reasonable, then generate a sample of adult weights by choosing a random value from the distribution of birth weights, choosing a sequence of factors from the distribution of `f`, and computing the product. What value of `n` is needed to converge to a lognormal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1.01\n",
    "sigma = 0.01\n",
    "\n",
    "sample = np.random.normal(mu, sigma, 1000)\n",
    "percent_gain = (sample-1) * 100\n",
    "Cdf.from_seq(percent_gain).plot()\n",
    "\n",
    "decorate(xlabel='Percent gain',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_life():\n",
    "    w0 = 3\n",
    "    rel_gains = np.random.normal(mu, sigma, 320)\n",
    "    weight = w0 * rel_gains.prod()\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_weights = np.array([simulate_life() for _ in range(1000)])\n",
    "sim_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(sim_weights, label='Simulated weights (kg)')\n",
    "\n",
    "decorate(xlabel='Adult weight',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_plot(sim_weights)\n",
    "\n",
    "decorate(xlabel='Random Gaussian values',\n",
    "         ylabel='Simulated weights (kg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of File Sizes\n",
    "\n",
    "The following command walks the entire file system (or at least the parts we have permission to read) and collects the sizes of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find / -type f -printf \"%s\\n\" > file_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head file_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's how we can read them into a Pandas `Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('file_sizes', header=None)\n",
    "df.columns = ['File size']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sizes = df['File size']\n",
    "file_sizes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see whether they might be lognormal, I'll compute their logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sizes[file_sizes == 0] = np.nan\n",
    "#file_sizes[file_sizes > 1e13] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_sizes = np.log10(file_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the CDF of the transformed sizes looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(log_file_sizes, label='Data')\n",
    "\n",
    "decorate(xlabel='Log file size (log B)',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_plot(log_file_sizes)\n",
    "\n",
    "decorate(xlabel='Random Gaussian values',\n",
    "         ylabel='Log file sizes (log B)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we think the distribution of file sizes might be lognormal?\n",
    "[Here's my answer](https://github.com/AllenDowney/ThinkComplexity2/raw/master/papers/downey01structural.pdf).\n",
    "\n",
    "Or maybe it follows a power law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = Cdf.from_seq(file_sizes)\n",
    "(1-cdf).plot()\n",
    "decorate(xscale='log',\n",
    "         yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Wealth\n",
    "\n",
    "A model like this is one of many possible explanations for the heavy-tailed distribution of wealth.\n",
    "\n",
    "Suppose everyone saves \\\\$2000 a year for 50 years. \n",
    "With no return on investment, they would have \\\\$100,000.\n",
    "\n",
    "But suppose they make 5% annually, on average, with variability from year to year. Let's see what the distribution of wealth looks like when they \"retire\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_wealth():\n",
    "    savings = 2000\n",
    "    total = 0\n",
    "    for i in range(50):\n",
    "        total *= np.random.normal(1.05, 0.05)\n",
    "        total += savings\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_wealth = np.array([simulate_wealth() for _ in range(1000)])\n",
    "sim_wealth.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, they have more than \\\\$400,000, which means that most of their wealth came from return on investment, not savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the distributions looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sim_wealth = np.log10(sim_wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(log_sim_wealth, label='Total wealth')\n",
    "\n",
    "decorate(xlabel='Simulated wealth',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_probability_plot(log_sim_wealth)\n",
    "\n",
    "decorate(xlabel='Random Gaussian values',\n",
    "         ylabel='Simulated weights (kg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this example generates a lognormal distribution, the mechanism here -- variation in return on investment -- is not the only source of variation in wealth. And it's probably not the biggest. We'll come back to this topic in Chapter 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mandelbrot's Hypothesis\n",
    "\n",
    "From Chapter 8 of [Think Complexity](https://greenteapress.com/complexity2/html/thinkcomplexity2009.html#sec83):\n",
    "\n",
    "> In *The Fractal Geometry of Nature*, Benoit Mandelbrot proposes what he calls a \"heretical\" explanation for the prevalence of heavy-tailed distributions in natural systems. It may not be, as Bak suggests, that many systems can generate this behavior in isolation. Instead there may be only a few, but interactions between systems might cause the behavior to propagate.\n",
    ">\n",
    "> To support this argument, Mandelbrot points out:\n",
    ">\n",
    "> *    The distribution of observed data is often \"the joint effect of a fixed underlying true distribution and a highly variable filter\".\n",
    "> *    Heavy-tailed distributions are robust to filtering; that is, \"a wide variety of filters leave their asymptotic behavior unchanged\".\n",
    "\n",
    "This argument is pretty abstract, since it does not specify what \"joint effect\" or \"variable filter\" are meant to correspond to in the world.\n",
    "\n",
    "But one kind of filter he might have had in mind is convolution, which is the operation that computes the distribution of a sum.\n",
    "Continuing the wealth example, suppose wealth is the sum of several factors and only one of them is heavy tailed.\n",
    "\n",
    "What does it look like if we add a Pareto distribution and a Gaussian distribution with the same mean and standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "sample1 = np.random.pareto(0.5, size=size)\n",
    "mean, std = sample1.mean(), sample1.std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = np.random.normal(mean, std, size=size)\n",
    "sample2.mean(), sample2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sample1 + sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_cdf(total, label='Total wealth')\n",
    "\n",
    "decorate(xlabel='Simulated wealth',\n",
    "         ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf1 = Cdf.from_seq(sample1)\n",
    "(1-cdf1).plot(label='Pareto')\n",
    "\n",
    "cdf2 = Cdf.from_seq(sample2)\n",
    "(1-cdf2).plot(label='Gaussian')\n",
    "\n",
    "cdf_total = Cdf.from_seq(total)\n",
    "(1-cdf_total).plot(label='Total')\n",
    "decorate(xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the challenges of working with the Pareto distribution is that the results are highly variable, even with large sample sizes. So you might want to run this example a few times, maybe with some larger values of `size`.\n",
    "\n",
    "What can we say about the tail behavior of `total`?\n",
    "\n",
    "And what does this imply about Mandelbrot's explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
